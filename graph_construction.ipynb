{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import functions as f\n",
    "from tqdm import tqdm\n",
    "\n",
    "TTN_BIN = 4275\n",
    "TTN_BINS = [4275, 4276, 4277, 4278]\n",
    "chrom2_bins = [2490, 4911]\n",
    "\n",
    "contact_matrix_zero = np.load('/Users/ethan/Desktop/chromatin-heart-dev/samples/contact_matrix_100kb_balanced_zeroed.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LOAD BIN MAP\n",
    "bin_map = f.load_bin_map('/Users/ethan/Desktop/chromatin-heart-dev/data/bin_map_human_100000.bed')\n",
    "\n",
    "gene_bins = []\n",
    "with open('/Users/ethan/Desktop/chromatin-heart-dev/data/gene_bins.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        gene_bins.append(line.strip())\n",
    "gene_bins = [int(x) for x in gene_bins]\n",
    "\n",
    "\n",
    "non_gene_bins = []\n",
    "with open('/Users/ethan/Desktop/chromatin-heart-dev/data/non_gene_bins.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        non_gene_bins.append(line.strip())\n",
    "non_gene_bins = [int(x) for x in non_gene_bins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_walk_index(contact_matrix):\n",
    "    \"\"\"\n",
    "    Precompute for each node:\n",
    "      - neighbors[i]: 1D int array of neighbors\n",
    "      - cdfs[i]:      1D float array of cumulative probabilities\n",
    "    \"\"\"\n",
    "    N = contact_matrix.shape[0]\n",
    "    neighbors = [None]*N\n",
    "    cdfs      = [None]*N\n",
    "\n",
    "    for i in tqdm(range(N)):\n",
    "        w = contact_matrix[i]\n",
    "        idx = np.nonzero(w)[0]\n",
    "        if idx.size == 0:\n",
    "            neighbors[i] = np.empty(0, dtype=int)\n",
    "            cdfs[i]      = np.empty(0, dtype=float)\n",
    "        else:\n",
    "            probs = w[idx] / w[idx].sum()\n",
    "            neighbors[i] = idx\n",
    "            cdfs[i]      = np.cumsum(probs)\n",
    "    return neighbors, cdfs\n",
    "\n",
    "neighbors, cdfs = build_walk_index(contact_matrix_zero) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_matrix = np.load('/Users/ethan/Desktop/chromatin-heart-dev/samples/contact_matrix_100kb_balanced.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding TTN Clique Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTN_BIN = 4275\n",
    "ttn_clique = f.find_clique_greedy(contact_matrix, 5, TTN_BIN, bin_map)\n",
    "G_TTN = f.clique_to_graph(contact_matrix, ttn_clique, TTN_BIN)\n",
    "score_greedy = f.calculate_avg_interaction_strength(contact_matrix, ttn_clique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttn_clique, score_greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eig\n",
    "import math\n",
    "\n",
    "def analytical_clique(contact_matrix: np.ndarray, n: int):\n",
    "    # 1) Build P so that P[i,j] = prob of moving i -> j\n",
    "    row_sums = contact_matrix.sum(axis=1)\n",
    "    P = np.zeros_like(contact_matrix, dtype=float)\n",
    "    for i, s in enumerate(row_sums):\n",
    "        if s > 0:\n",
    "            P[i, :] = contact_matrix[i, :] / s\n",
    "        else:\n",
    "            # no outgoing edges: stay in place\n",
    "            P[i, i] = 1.0\n",
    "\n",
    "    # 2) Compute left eigenvectors of P (i.e., eigenvectors of P^T)\n",
    "    #    eig returns (w, vl) where vl[:,k] is the left eigenvector for w[k]\n",
    "    w, vl = eig(P, left=True, right=False)\n",
    "\n",
    "    # 3) Find the eigenvalue closest to 1\n",
    "    idx = int(np.argmin(np.abs(w - 1.0)))\n",
    "    pi = vl[:, idx].real  # take real part\n",
    "\n",
    "    # 4) Normalize to sum to 1\n",
    "    pi = pi / np.sum(pi)\n",
    "\n",
    "    # 5) Pick top-n\n",
    "    clique = np.argsort(pi)[-n:][::-1]\n",
    "    return clique, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def analytical_diffusion_clique(contact_matrix: np.ndarray,\n",
    "                                start_node: int,\n",
    "                                n: int,\n",
    "                                alpha: float = 0.1):\n",
    "\n",
    "    N = contact_matrix.shape[0]\n",
    "\n",
    "    # 1) Build the row‑stochastic transition matrix P\n",
    "    P = np.zeros((N, N), dtype=float)\n",
    "    row_sums = contact_matrix.sum(axis=1)\n",
    "    for i in range(N):\n",
    "        if row_sums[i] > 0:\n",
    "            P[i, :] = contact_matrix[i, :] / row_sums[i]\n",
    "        else:\n",
    "            # no neighbors → self‑loop\n",
    "            P[i, i] = 1.0\n",
    "\n",
    "    # 2) Form the fundamental matrix: F = (I - (1-alpha)*P)^(-1)\n",
    "    I = np.eye(N)\n",
    "    F = np.linalg.inv(I - (1 - alpha) * P)\n",
    "\n",
    "    # 3) Extract the expected visits for a start at `start_node`\n",
    "    visits = F[start_node, :]\n",
    "\n",
    "    # 4) Pick the top‑n nodes by descending visits\n",
    "    clique = np.argsort(visits)[-n:][::-1]\n",
    "    return clique, visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "from time import time   \n",
    "\n",
    "# print time taken\n",
    "def sparse_diffusion_clique(contact_matrix, start_node, n, alpha=0.1, rtol=1e-6):\n",
    "    # 1) Ensure CSR sparse matrix\n",
    "    if isinstance(contact_matrix, np.ndarray):\n",
    "        contact_matrix = sp.csr_matrix(contact_matrix)\n",
    "    else:\n",
    "        contact_matrix = contact_matrix.tocsr()\n",
    "\n",
    "    N = contact_matrix.shape[0]\n",
    "\n",
    "    # 2) Normalize rows → P\n",
    "    row_sums = np.array(contact_matrix.sum(axis=1)).ravel()\n",
    "    P = contact_matrix.tolil()\n",
    "    inv_row = np.zeros_like(row_sums, dtype=float)\n",
    "    nonzero = row_sums > 0\n",
    "    inv_row[nonzero] = 1.0 / row_sums[nonzero]\n",
    "\n",
    "    for i in range(N):\n",
    "        if nonzero[i]:\n",
    "            P.data[i] = [val * inv_row[i] for val in P.data[i]]\n",
    "        else:\n",
    "            # self‑loop\n",
    "            P.rows[i] = [i]\n",
    "            P.data[i] = [1.0]\n",
    "\n",
    "    P = P.tocsr()\n",
    "\n",
    "    # 3) Build M = I - (1-α)P\n",
    "    M = sp.eye(N, format=\"csr\") - (1 - alpha) * P\n",
    "\n",
    "    # 4) Setup RHS e_s\n",
    "    e_s = np.zeros(N, float)\n",
    "    e_s[start_node] = 1.0\n",
    "\n",
    "    # 5) Solve (I - (1-α)P)^T v^T = e_s^T via GMRES\n",
    "    visits, info = spla.gmres(M.T, e_s, rtol=rtol)\n",
    "    if info != 0:\n",
    "        raise RuntimeError(f\"GMRES did not converge (info={info})\")\n",
    "\n",
    "    # 6) Top‑n\n",
    "    clique = np.argsort(visits)[-n:][::-1]\n",
    "\n",
    "    return clique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample matrix\n",
    "sample_matrix = f.generate_sample_matrix_bins(100)\n",
    "\n",
    "# visualize the contact matrix hic \n",
    "plt.imshow(sample_matrix, cmap='hot', interpolation='nearest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytical_diffusion_clique(sample_matrix, start_node=4, n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.random_walk(sample_matrix, start_node=4, n=6, num_molecules=100000, alpha=0.1, verbose=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_diffusion_clique(sample_matrix, start_node=4, n=6, alpha=0.1, rtol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sizes = [100, 1000, 5000, 10000]\n",
    "results = []\n",
    "\n",
    "for N in sizes:\n",
    "    # 1) generate\n",
    "    mat = f.generate_sample_matrix_bins(N)\n",
    "    \n",
    "    row = {'N': N}\n",
    "    \n",
    "    # 2) analytical\n",
    "    t0 = time.perf_counter()\n",
    "    analytical_diffusion_clique(mat, start_node=4, n=6)\n",
    "    row['analytical_s'] = time.perf_counter() - t0\n",
    "    \n",
    "    # 3) random walk\n",
    "    t0 = time.perf_counter()\n",
    "    f.random_walk(mat, start_node=4, n=6, num_molecules=10000, alpha=0.1, verbose=False)\n",
    "    row['random_walk_s'] = time.perf_counter() - t0\n",
    "    \n",
    "    # 4) sparse\n",
    "    t0 = time.perf_counter()\n",
    "    sparse_diffusion_clique(mat, start_node=4, n=6, alpha=0.1, rtol=1e-6)\n",
    "    row['sparse_s'] = time.perf_counter() - t0\n",
    "    \n",
    "    results.append(row)\n",
    "\n",
    "# 5) summarize\n",
    "df = pd.DataFrame(results).set_index('N')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# zero out cm if not above 2\n",
    "sample_matrix = np.where(contact_matrix < 2, 0, sample_matrix)\n",
    "\n",
    "# plot this on a graph\n",
    "G = f.construct_graph_from_contact_matrix(sample_matrix, threshold=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Walk Clique Size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nodes = f.random_walk(contact_matrix_zero, TTN_BIN, 5, num_molecules=1000, alpha=0.05, verbose=True) \n",
    "rw_score_5  = f.calculate_avg_interaction_strength(contact_matrix_zero, top_nodes)\n",
    "rw_score_5  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_model_random_walk_5 = f.create_background_model_rw(contact_matrix_zero, 5, num_molecules=1000, num_iterations=100, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.simple_p_test(rw_score_5, bg_model_random_walk_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_score_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Walk Clique Size 5 with 100 molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_model_random_walk_sample = f.create_background_model_rw(contact_matrix_zero, 5, num_molecules=100, num_iterations=1000, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nodes = f.random_walk(contact_matrix_zero, TTN_BIN, 5, num_molecules=100, alpha=0.05, verbose=True) \n",
    "rw_score_5  = f.calculate_avg_interaction_strength(contact_matrix_zero, top_nodes)\n",
    "rw_score_5  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.simple_p_test(rw_score_5, bg_model_random_walk_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [ 4277, 30364, 16661,  6802,  4876]\n",
    "f.clique_to_graph(contact_matrix_zero, nodes, TTN_BIN)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Walk Clique Size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nodes = f.random_walk(contact_matrix_zero, TTN_BIN, 10, num_molecules=1000, alpha=0.05, verbose=True) \n",
    "rw_score_10  = f.calculate_avg_interaction_strength(contact_matrix_zero, top_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_model_random_walk_10 = f.create_background_model_rw(contact_matrix_zero, 10, num_molecules=1000, num_iterations=100, alpha=0.05)\n",
    "# save list of scores in /background_models\n",
    "\n",
    "with open('/Users/ethan/Desktop/chromatin-heart-dev/background_models/rw_scores_10.txt', 'w') as f:\n",
    "    for item in bg_model_random_walk_10:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.simple_p_test(rw_score_10, bg_model_random_walk_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_score_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RW Clique Size 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_model_random_walk_15 = f.create_background_model_rw(contact_matrix_zero, 15, num_molecules=1000, num_iterations=1000, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nodes_rw_15 = f.random_walk(contact_matrix_zero, TTN_BIN, 15, num_molecules=1000, alpha=0.05, verbose=True) \n",
    "rw_score_15  = f.calculate_avg_interaction_strength(contact_matrix_zero, top_nodes)\n",
    "print(top_nodes_rw_15)\n",
    "print(rw_score_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as f\n",
    "f.simple_p_test(rw_score_15, bg_model_random_walk_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Clique Size to P Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def t_test(observed_score, score_list):\n",
    "    t_stat, p_value = stats.ttest_1samp(score_list, observed_score)\n",
    "    return p_value\n",
    "\n",
    "\n",
    "def mann_whitney_u_test(ttn_score, random_scores):\n",
    "    ttn_array = np.array([ttn_score] * len(random_scores)) \n",
    "    u_statistic, p_value = stats.mannwhitneyu(ttn_array, random_scores, alternative='greater')\n",
    "    return p_value\n",
    "\n",
    "def empirical_p_value(observed_score, random_scores):\n",
    "    count = sum(score >= observed_score for score in random_scores)\n",
    "    p_value = count / len(random_scores)\n",
    "    return p_value\n",
    "\n",
    "    \"\"\"\n",
    "    Compares an observed score to a list of scores using a permutation test.\n",
    "    \"\"\"\n",
    "    combined_scores = np.concatenate([score_list, [observed_score]])\n",
    "    n_observed = 1\n",
    "    n_other = len(score_list)\n",
    "\n",
    "    observed_statistic = observed_score # Or another relevant statistic\n",
    "\n",
    "    extreme_count = 0\n",
    "    for _ in range(n_permutations):\n",
    "        permuted_indices = np.random.permutation(len(combined_scores))\n",
    "        permuted_observed_score = combined_scores[permuted_indices[-1]] # Assume observed score is the last one\n",
    "\n",
    "        permuted_statistic = permuted_observed_score # Use the same statistic\n",
    "\n",
    "        # Adjust the comparison based on the direction of significance you're looking for\n",
    "        # For \"more significant\" (assuming higher score), check if permuted statistic is as extreme or more extreme\n",
    "        if permuted_statistic >= observed_statistic:\n",
    "            extreme_count += 1\n",
    "\n",
    "    p_value = extreme_count / n_permutations\n",
    "    return p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTN_BIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "clique_sizes = [3, 4, 5, 8, 10, 12, 15, 20, 40, 60]\n",
    "p_values = []\n",
    "\n",
    "for i in clique_sizes:\n",
    "    top_nodes = f.find_clique_greedy(contact_matrix_zero, i, TTN_BIN, bin_map)\n",
    "    greedy_score = f.calculate_avg_interaction_strength(contact_matrix_zero, top_nodes)\n",
    "    bg_model_greedy = f.create_background_model_greedy_strong(contact_matrix_zero, i, bin_map, gene_bins=gene_bins, num_iterations=1000)\n",
    "    \n",
    "    p_value = mann_whitney_u_test(greedy_score, bg_model_greedy) \n",
    "    p_values.append(p_value)\n",
    "\n",
    "plt.plot(clique_sizes, p_values, marker='o')\n",
    "plt.xlabel('Clique Size')\n",
    "plt.ylabel('p-value')\n",
    "plt.yscale('log')  # Useful if values vary widely\n",
    "plt.axhline(0.05, color='r', linestyle='--', label='Significance Threshold (0.05)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Plot of Backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTN_BIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = f.find_clique_greedy(contact_matrix_zero, 5, 4276, bin_map)\n",
    "f.clique_to_graph(contact_matrix_zero, test, TTN_BIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.calculate_avg_interaction_strength(contact_matrix_zero, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clique_sizes = [5, 10, 20, 40]\n",
    "p_values = []\n",
    "num_iterations = 1000\n",
    "ttn_bins = [4275, 4276, 4277, 4278]\n",
    "colors = ['red', 'blue', 'green', 'orange']  # Define different colors for each bin\n",
    "\n",
    "for i in clique_sizes:\n",
    "    ttn_cliques = [f.find_clique_greedy(contact_matrix_zero, i, ttn_bin, bin_map) for ttn_bin in ttn_bins]\n",
    "    ttn_scores = [f.calculate_avg_interaction_strength(contact_matrix_zero, ttn_clique) for ttn_clique in ttn_cliques]\n",
    "    \n",
    "    bg_model_greedy_weak = f.create_background_model_greedy(contact_matrix_zero, i, bin_map, non_gene_bins, label='weak', num_iterations=num_iterations, display=False)\n",
    "    bg_model_greedy_strong = f.create_background_model_greedy(contact_matrix_zero, i, bin_map, gene_bins, label='strong', num_iterations=num_iterations, display=False)\n",
    "\n",
    "    # Create subplots: 1 row, 2 columns\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot weak model\n",
    "    axes[0].hist(bg_model_greedy_weak, bins=50, color='skyblue', edgecolor='black')\n",
    "    for idx, (ttn_score, ttn_bin) in enumerate(zip(ttn_scores, ttn_bins)):\n",
    "        axes[0].axvline(x=ttn_score, color=colors[idx], linestyle='--', label=f'TTN Score {ttn_bin}: {ttn_score}')\n",
    "    axes[0].set_xlabel('Average Interaction Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title(f'Distribution of AIS using Greedy Weak for {num_iterations} Weak Cliques of Size {i}')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot strong model\n",
    "    axes[1].hist(bg_model_greedy_strong, bins=50, color='skyblue', edgecolor='black')\n",
    "    for idx, (ttn_score, ttn_bin) in enumerate(zip(ttn_scores, ttn_bins)):\n",
    "        axes[1].axvline(x=ttn_score, color=colors[idx], linestyle='--', label=f'TTN Score {ttn_bin}: {ttn_score}')\n",
    "    axes[1].set_xlabel('Average Interaction Score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'Distribution of AIS using Greedy for {num_iterations} Strong Cliques of Size {i}')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clique_sizes = [20]\n",
    "p_values = []\n",
    "num_iterations = 1000\n",
    "ttn_bins = [4275, 4276, 4277, 4278]\n",
    "colors = ['red', 'blue', 'green', 'orange']  # Define different colors for each bin\n",
    "\n",
    "for i in clique_sizes:\n",
    "    ttn_cliques = [f.find_clique_greedy(contact_matrix_zero, i, ttn_bin, bin_map) for ttn_bin in ttn_bins]\n",
    "    ttn_scores = [f.calculate_avg_interaction_strength(contact_matrix_zero, ttn_clique) for ttn_clique in ttn_cliques]\n",
    "    \n",
    "    bg_model_greedy_weak = f.create_background_model_greedy(contact_matrix_zero, i, bin_map, non_gene_bins, label='weak', num_iterations=num_iterations, display=False)\n",
    "    bg_model_greedy_strong = f.create_background_model_greedy(contact_matrix_zero, i, bin_map, gene_bins, label='strong', num_iterations=num_iterations, display=False)\n",
    "    \n",
    "    # Create subplots: 1 row, 2 columns\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot weak model\n",
    "    axes[0].hist(bg_model_greedy_weak, bins=50, color='skyblue', edgecolor='black')\n",
    "    for idx, (ttn_score, ttn_bin) in enumerate(zip(ttn_scores, ttn_bins)):\n",
    "        axes[0].axvline(x=ttn_score, color=colors[idx], linestyle='--', label=f'TTN Score {ttn_bin}: {ttn_score}')\n",
    "    axes[0].set_xlabel('Average Interaction Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title(f'Distribution of AIS using Greedy Weak for {num_iterations} Weak Cliques of Size {i}')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot strong model\n",
    "    axes[1].hist(bg_model_greedy_strong, bins=50, color='skyblue', edgecolor='black')\n",
    "    for idx, (ttn_score, ttn_bin) in enumerate(zip(ttn_scores, ttn_bins)):\n",
    "        axes[1].axvline(x=ttn_score, color=colors[idx], linestyle='--', label=f'TTN Score {ttn_bin}: {ttn_score}')\n",
    "    axes[1].set_xlabel('Average Interaction Score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'Distribution of AIS using Greedy for {num_iterations} Strong Cliques of Size {i}')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clique_sizes = [2]\n",
    "p_values = []\n",
    "num_iterations = 500\n",
    "ttn_bins = [4275, 4276, 4277, 4278]\n",
    "colors = ['red', 'blue', 'green', 'orange']  # Define different colors for each bin\n",
    "\n",
    "for i in clique_sizes:\n",
    "    ttn_cliques = [f.find_clique_greedy(contact_matrix_zero, i, ttn_bin, bin_map) for ttn_bin in ttn_bins]\n",
    "    ttn_scores = [f.calculate_avg_interaction_strength(contact_matrix_zero, ttn_clique) for ttn_clique in ttn_cliques]\n",
    "    \n",
    "    bg_model_greedy_weak = f.create_background_model_greedy(contact_matrix_zero, i, bin_map, non_gene_bins, label='weak', num_iterations=num_iterations, display=False)\n",
    "    bg_model_greedy_strong = f.create_background_model_greedy(contact_matrix_zero, i, bin_map, gene_bins, label='strong', num_iterations=num_iterations, display=False)\n",
    "\n",
    "    # Create subplots: 1 row, 2 columns\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot weak model\n",
    "    axes[0].hist(bg_model_greedy_weak, bins=50, color='skyblue', edgecolor='black')\n",
    "    for idx, (ttn_score, ttn_bin) in enumerate(zip(ttn_scores, ttn_bins)):\n",
    "        axes[0].axvline(x=ttn_score, color=colors[idx], linestyle='--', label=f'TTN Score {ttn_bin}: {ttn_score}')\n",
    "    axes[0].set_xlabel('Average Interaction Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title(f'Distribution of AIS using Greedy Weak for {num_iterations} Weak Cliques of Size {i}')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot strong model\n",
    "    axes[1].hist(bg_model_greedy_strong, bins=50, color='skyblue', edgecolor='black')\n",
    "    for idx, (ttn_score, ttn_bin) in enumerate(zip(ttn_scores, ttn_bins)):\n",
    "        axes[1].axvline(x=ttn_score, color=colors[idx], linestyle='--', label=f'TTN Score {ttn_bin}: {ttn_score}')\n",
    "    axes[1].set_xlabel('Average Interaction Score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'Distribution of AIS using Greedy for {num_iterations} Strong Cliques of Size {i}')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violin Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "clique_sizes = [5, 10, 20]\n",
    "\n",
    "bg_scores_list = []\n",
    "for size in clique_sizes:\n",
    "    path = f'/Users/ethan/Desktop/chromatin-heart-dev/background_models/greedy_scores_{size}_iterations_1000_strong.txt'\n",
    "    with open(path, 'r') as file:\n",
    "        scores = [float(line) for line in file]\n",
    "    bg_scores_list.append(scores)\n",
    "\n",
    "# Create the violin plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.violinplot(data=bg_scores_list, inner=\"quartile\", cut=0)\n",
    "plt.xticks(ticks=range(len(clique_sizes)), labels=clique_sizes)\n",
    "plt.xlabel('Clique Size')\n",
    "plt.ylabel('Background Score Distribution')\n",
    "plt.title('Violin Plot of Greedy‐Score Strong Background Distributions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "clique_sizes = [5, 10, 20]\n",
    "\n",
    "bg_scores_list = []\n",
    "for size in clique_sizes:\n",
    "    path = f'/Users/ethan/Desktop/chromatin-heart-dev/background_models/greedy_scores_{size}_iterations_1000_weak.txt'\n",
    "    with open(path, 'r') as f:\n",
    "        scores = [float(line) for line in f]\n",
    "    bg_scores_list.append(scores)\n",
    "\n",
    "# Create the violin plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.violinplot(data=bg_scores_list, inner=\"quartile\", cut=0)\n",
    "plt.xticks(ticks=range(len(clique_sizes)), labels=clique_sizes)\n",
    "plt.xlabel('Clique Size')\n",
    "plt.ylabel('Background Score Distribution')\n",
    "plt.title('Violin Plot of Greedy‐Score Weak Background Distributions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "clique_sizes = [5, 10, 20]\n",
    "records = []\n",
    "\n",
    "for model_type in [\"strong\", \"weak\"]:\n",
    "    for size in clique_sizes:\n",
    "        path = (f\"/Users/ethan/Desktop/chromatin-heart-dev/\"\n",
    "                f\"background_models/greedy_scores_{size}_iterations_1000_{model_type}.txt\")\n",
    "        with open(path, \"r\") as f:\n",
    "            scores = [float(line) for line in f]\n",
    "        for s in scores:\n",
    "            records.append({\n",
    "                \"Clique Size\": size,\n",
    "                \"Score\": s,\n",
    "                \"Model\": \"Strong\" if model_type == \"strong\" else \"Weak\"\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# 2) Plot a split‐violin\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.violinplot(\n",
    "    data=df,\n",
    "    x=\"Clique Size\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Model\",\n",
    "    split=True,            # split each violin in half\n",
    "    inner=\"quartile\",      # show median + quartiles\n",
    "    cut=0                  # don’t extend past data\n",
    ")\n",
    "sns.despine(left=True)\n",
    "plt.title(\"Strong vs. Weak Model Background Distributions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "clique_sizes = [5, 10, 20]\n",
    "records = []\n",
    "for model_type in [\"strong\", \"weak\"]:\n",
    "    for size in clique_sizes:\n",
    "        path = (\n",
    "            f\"/Users/ethan/Desktop/chromatin-heart-dev/\"\n",
    "            f\"background_models/greedy_scores_{size}_iterations_1000_{model_type}.txt\"\n",
    "        )\n",
    "        with open(path, \"r\") as f:\n",
    "            scores = [float(line) for line in f]\n",
    "        for s in scores:\n",
    "            records.append({\n",
    "                \"Clique Size\": size,\n",
    "                \"Score\": s,\n",
    "                \"Model\": \"Strong\" if model_type==\"strong\" else \"Weak\"\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# ——— Plot with extra width ———\n",
    "plt.figure(figsize=(14, 5))    # make it wider!\n",
    "ax = sns.violinplot(\n",
    "    data=df,\n",
    "    x=\"Clique Size\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Model\",\n",
    "    dodge=True,\n",
    "    width=0.7,                # controls how “fat” each violin is\n",
    "    inner=\"quartile\",\n",
    "    cut=0\n",
    ")\n",
    "sns.despine(left=True)\n",
    "ax.set_yscale('symlog', linthresh=0.01)\n",
    "plt.title(\"Strong vs. Weak Distributions (wider figure)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Violin Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD BIN MAP\n",
    "bin_map_loc = f.load_bin_map_loc('/Users/ethan/Desktop/chromatin-heart-dev/data/bin_map_human_100000.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_top_n_tf_clique(clique_size):\n",
    "    # Load the transcription factor binding sites\n",
    "    data_dir = './data/transcription_factors'\n",
    "    tf_files = os.listdir(data_dir)\n",
    "    interaction_scores = []\n",
    "\n",
    "    # Contains top 1 bin for each TF\n",
    "    max_bins = []\n",
    "    bin_set = set()\n",
    "\n",
    "    # Return counts of transcription factor binding sites in each 100kb bin\n",
    "    def process_bed_file(file_path):\n",
    "        bin_counts = {}\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.startswith('#') or not line.strip():\n",
    "                    continue\n",
    "                columns = line.split()\n",
    "                \n",
    "                chrom = columns[0]\n",
    "                start_position = int(columns[1])    \n",
    "                \n",
    "                bin_number = f.find_bin(chrom, start_position, bin_map_loc)\n",
    "                \n",
    "                if bin_number not in bin_counts:\n",
    "                    bin_counts[bin_number] = 0\n",
    "                bin_counts[bin_number] += 1\n",
    "\n",
    "        return bin_counts\n",
    "\n",
    "\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        # open file\n",
    "\n",
    "        try:    \n",
    "            bin_counts = process_bed_file(os.path.join(data_dir, filename))\n",
    "        except:\n",
    "            print(f'Error processing {filename}')\n",
    "            continue\n",
    "\n",
    "        # get top 20 bins\n",
    "        top_bins = sorted(bin_counts, key=bin_counts.get, reverse=True)[:clique_size]\n",
    "\n",
    "        # print the top bins and corresponding counts\n",
    "        print(f'Transcription factor: {filename}')\n",
    "        print(f'Top {clique_size} bins: {top_bins}')\n",
    "        print(f'Counts: {[bin_counts[bin] for bin in top_bins]}')\n",
    "\n",
    "        max_bins.append(top_bins[0])\n",
    "        bin_set.update(top_bins)\n",
    "\n",
    "        interaction_strength = f.calculate_avg_interaction_strength(contact_matrix_zero, top_bins)\n",
    "        interaction_scores.append(interaction_strength)\n",
    "\n",
    "    return interaction_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_middle_n_tf_clique(clique_size):\n",
    "    # Load the transcription factor binding sites\n",
    "    data_dir = './data/transcription_factors'\n",
    "    tf_files = os.listdir(data_dir)\n",
    "    interaction_scores = []\n",
    "\n",
    "    # Contains middle bins for each TF\n",
    "    middle_bins = []\n",
    "    bin_set = set()\n",
    "\n",
    "    # Return counts of transcription factor binding sites in each 100kb bin\n",
    "    def process_bed_file(file_path):\n",
    "        bin_counts = {}\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.startswith('#') or not line.strip():\n",
    "                    continue\n",
    "                columns = line.split()\n",
    "                \n",
    "                chrom = columns[0]\n",
    "                start_position = int(columns[1])    \n",
    "                \n",
    "                bin_number = f.find_bin(chrom, start_position, bin_map_loc)\n",
    "                \n",
    "                if bin_number not in bin_counts:\n",
    "                    bin_counts[bin_number] = 0\n",
    "                bin_counts[bin_number] += 1\n",
    "\n",
    "        return bin_counts\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        try:    \n",
    "            bin_counts = process_bed_file(os.path.join(data_dir, filename))\n",
    "        except:\n",
    "            print(f'Error processing {filename}')\n",
    "            continue\n",
    "\n",
    " \n",
    "        sorted_bins = sorted(bin_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        total_bins = len(sorted_bins)\n",
    "        \n",
    "        start_index = (total_bins - clique_size) // 2\n",
    "        if start_index < 0:  \n",
    "            start_index = 0\n",
    "        \n",
    "        middle_n_bins = [bin_num for bin_num, _ in sorted_bins[start_index:start_index + clique_size]]\n",
    "\n",
    "        # print middle n bins and corresponding counts\n",
    "        print(f'Transcription factor: {filename}')\n",
    "        print(f'Middle {clique_size} bins: {middle_n_bins}')\n",
    "        print(f'Counts: {[bin_counts[bin] for bin in middle_n_bins]}')\n",
    "        \n",
    "        \n",
    "        if len(middle_n_bins) < clique_size:\n",
    "            middle_n_bins = [bin_num for bin_num, _ in sorted_bins]\n",
    "        \n",
    "        if middle_n_bins: \n",
    "            middle_bins.append(middle_n_bins[0])\n",
    "            bin_set.update(middle_n_bins)\n",
    "\n",
    "            interaction_strength = f.calculate_avg_interaction_strength(contact_matrix_zero, middle_n_bins)\n",
    "            interaction_scores.append(interaction_strength)\n",
    "\n",
    "    return interaction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as f\n",
    "\n",
    "# do greedy on the top bins of each transcription factor\n",
    "def bg_greedy_top_bin_tf(contact_matrix_zero, max_bins, clique_size, bin_map):\n",
    "    interaction_scores = []\n",
    "    for bin in tqdm(max_bins):\n",
    "        top_nodes = f.find_clique_greedy(contact_matrix_zero, clique_size, bin, bin_map)\n",
    "        greedy_score  = f.calculate_avg_interaction_strength(contact_matrix_zero, top_nodes)\n",
    "        interaction_scores.append(greedy_score)\n",
    "\n",
    "    return interaction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = get_middle_n_tf_clique(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = get_top_n_tf_clique(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(test1), np.mean(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import functions as f\n",
    "\n",
    "def get_top_n_tf_clique(clique_size):\n",
    "    # Load the transcription factor binding sites\n",
    "    data_dir = './data/transcription_factors'\n",
    "    tf_files = os.listdir(data_dir)\n",
    "    interaction_scores = []\n",
    "\n",
    "    # Contains top 1 bin for each TF\n",
    "    max_bins = []\n",
    "    bin_set = set()\n",
    "\n",
    "    # Return counts of transcription factor binding sites in each 100kb bin\n",
    "    def process_bed_file(file_path):\n",
    "        bin_counts = {}\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.startswith('#') or not line.strip():\n",
    "                    continue\n",
    "                columns = line.split()\n",
    "                \n",
    "                chrom = columns[0]\n",
    "                start_position = int(columns[1])    \n",
    "                \n",
    "                bin_number = f.find_bin(chrom, start_position, bin_map_loc)\n",
    "                \n",
    "                if bin_number not in bin_counts:\n",
    "                    bin_counts[bin_number] = 0\n",
    "                bin_counts[bin_number] += 1\n",
    "\n",
    "        return bin_counts\n",
    "\n",
    "\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        # open file\n",
    "\n",
    "        try:    \n",
    "            bin_counts = process_bed_file(os.path.join(data_dir, filename))\n",
    "        except:\n",
    "            print(f'Error processing {filename}')\n",
    "            continue\n",
    "\n",
    "        # get top 20 bins\n",
    "        top_bins = sorted(bin_counts, key=bin_counts.get, reverse=True)[:clique_size]\n",
    "\n",
    "        max_bins.append(top_bins[0])\n",
    "        bin_set.update(top_bins)\n",
    "\n",
    "        interaction_strength = f.calculate_avg_interaction_strength(contact_matrix_zero, top_bins)\n",
    "        interaction_scores.append(interaction_strength)\n",
    "\n",
    "    return interaction_scores\n",
    "    \n",
    "# do greedy on the top bins of each transcription factor\n",
    "def bg_greedy_top_bin_tf(contact_matrix_zero, max_bins, clique_size, bin_map):\n",
    "    interaction_scores = []\n",
    "    for bin in tqdm(max_bins):\n",
    "        top_nodes = f.find_clique_greedy(contact_matrix_zero, clique_size, bin, bin_map)\n",
    "        greedy_score  = f.calculate_avg_interaction_strength(contact_matrix_zero, top_nodes)\n",
    "        interaction_scores.append(greedy_score)\n",
    "\n",
    "    return interaction_scores\n",
    "\n",
    "\n",
    "\n",
    "# ——— parameters & record collection ———\n",
    "clique_sizes = [5, 10, 20]\n",
    "records = []\n",
    "\n",
    "# 1) load your precomputed “strong”/“weak” scores from disk\n",
    "for model_type in [\"strong\", \"weak\"]:\n",
    "    label = \"Strong\" if model_type == \"strong\" else \"Weak\"\n",
    "    for size in clique_sizes:\n",
    "        path = (\n",
    "            f\"/Users/ethan/Desktop/chromatin-heart-dev/\"\n",
    "            f\"background_models/greedy_scores_{size}_iterations_1000_{model_type}.txt\"\n",
    "        )\n",
    "        with open(path, \"r\") as fh:\n",
    "            scores = [float(line) for line in fh]\n",
    "        for s in scores:\n",
    "            records.append({\n",
    "                \"Clique Size\": size,\n",
    "                \"Score\": s,\n",
    "                \"Model\": label\n",
    "            })\n",
    "\n",
    "# 2) now call your TF‐function for each clique size and append those too\n",
    "for size in clique_sizes:\n",
    "    tf_scores = get_top_n_tf_clique(size)\n",
    "    for s in tf_scores:\n",
    "        records.append({\n",
    "            \"Clique Size\": size,\n",
    "            \"Score\": s,\n",
    "            \"Model\": \"TF\"\n",
    "        })\n",
    "\n",
    "# 3) call bg_greedy_top_bin_tf for each clique size and append those too\n",
    "for size in clique_sizes:\n",
    "    bg_scores = bg_greedy_top_bin_tf(contact_matrix_zero, max_bins, size, bin_map)\n",
    "    for s in bg_scores:\n",
    "        records.append({\n",
    "            \"Clique Size\": size,\n",
    "            \"Score\": s,\n",
    "            \"Model\": \"Greedy TF Top\"\n",
    "        })\n",
    "\n",
    "# build the DataFrame\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# ——— plot ———\n",
    "plt.figure(figsize=(14, 5))\n",
    "ax = sns.violinplot(\n",
    "    data=df,\n",
    "    x=\"Clique Size\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Model\",      # will now show Strong, Weak, TF\n",
    "    dodge=True,\n",
    "    width=0.7,\n",
    "    inner=\"quartile\",\n",
    "    cut=0\n",
    ")\n",
    "sns.despine(left=True)\n",
    "ax.set_yscale('symlog', linthresh=0.01)\n",
    "ax.set_title(\"Strong vs. Weak vs. TF Distributions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import functions as f\n",
    "\n",
    "def get_top_n_tf_clique(clique_size):\n",
    "    # Load the transcription factor binding sites\n",
    "    data_dir = './data/transcription_factors'\n",
    "    tf_files = os.listdir(data_dir)\n",
    "    interaction_scores = []\n",
    "\n",
    "    # Contains top 1 bin for each TF\n",
    "    max_bins = []\n",
    "    bin_set = set()\n",
    "\n",
    "    # Return counts of transcription factor binding sites in each 100kb bin\n",
    "    def process_bed_file(file_path):\n",
    "        bin_counts = {}\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.startswith('#') or not line.strip():\n",
    "                    continue\n",
    "                columns = line.split()\n",
    "                \n",
    "                chrom = columns[0]\n",
    "                start_position = int(columns[1])    \n",
    "                \n",
    "                bin_number = f.find_bin(chrom, start_position, bin_map_loc)\n",
    "                \n",
    "                if bin_number not in bin_counts:\n",
    "                    bin_counts[bin_number] = 0\n",
    "                bin_counts[bin_number] += 1\n",
    "\n",
    "        return bin_counts\n",
    "\n",
    "\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        # open file\n",
    "\n",
    "        try:    \n",
    "            bin_counts = process_bed_file(os.path.join(data_dir, filename))\n",
    "        except:\n",
    "            print(f'Error processing {filename}')\n",
    "            continue\n",
    "\n",
    "        # get top 20 bins\n",
    "        top_bins = sorted(bin_counts, key=bin_counts.get, reverse=True)[:clique_size]\n",
    "\n",
    "        max_bins.append(top_bins[0])\n",
    "        bin_set.update(top_bins)\n",
    "\n",
    "        interaction_strength = f.calculate_avg_interaction_strength(contact_matrix_zero, top_bins)\n",
    "        interaction_scores.append(interaction_strength)\n",
    "\n",
    "    return interaction_scores\n",
    "\n",
    "def get_middle_n_tf_clique(clique_size):\n",
    "    # Load the transcription factor binding sites\n",
    "    data_dir = './data/transcription_factors'\n",
    "    tf_files = os.listdir(data_dir)\n",
    "    interaction_scores = []\n",
    "\n",
    "    # Contains middle bins for each TF\n",
    "    middle_bins = []\n",
    "    bin_set = set()\n",
    "\n",
    "    # Return counts of transcription factor binding sites in each 100kb bin\n",
    "    def process_bed_file(file_path):\n",
    "        bin_counts = {}\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.startswith('#') or not line.strip():\n",
    "                    continue\n",
    "                columns = line.split()\n",
    "                \n",
    "                chrom = columns[0]\n",
    "                start_position = int(columns[1])    \n",
    "                \n",
    "                bin_number = f.find_bin(chrom, start_position, bin_map_loc)\n",
    "                \n",
    "                if bin_number not in bin_counts:\n",
    "                    bin_counts[bin_number] = 0\n",
    "                bin_counts[bin_number] += 1\n",
    "\n",
    "        return bin_counts\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        try:    \n",
    "            bin_counts = process_bed_file(os.path.join(data_dir, filename))\n",
    "        except:\n",
    "            print(f'Error processing {filename}')\n",
    "            continue\n",
    "\n",
    " \n",
    "        sorted_bins = sorted(bin_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        total_bins = len(sorted_bins)\n",
    "        \n",
    "        start_index = (total_bins - clique_size) // 2\n",
    "        if start_index < 0:  \n",
    "            start_index = 0\n",
    "        \n",
    "        middle_n_bins = [bin_num for bin_num, _ in sorted_bins[start_index:start_index + clique_size]]\n",
    "\n",
    "        \n",
    "        if len(middle_n_bins) < clique_size:\n",
    "            middle_n_bins = [bin_num for bin_num, _ in sorted_bins]\n",
    "        \n",
    "        if middle_n_bins: \n",
    "            middle_bins.append(middle_n_bins[0])\n",
    "            bin_set.update(middle_n_bins)\n",
    "\n",
    "            interaction_strength = f.calculate_avg_interaction_strength(contact_matrix_zero, middle_n_bins)\n",
    "            interaction_scores.append(interaction_strength)\n",
    "\n",
    "    return interaction_scores\n",
    "\n",
    "def bg_greedy_top_bin_tf(contact_matrix_zero, max_bins, clique_size, bin_map):\n",
    "    interaction_scores = []\n",
    "    for bin in tqdm(max_bins):\n",
    "        top_nodes = f.find_clique_greedy(contact_matrix_zero, clique_size, bin, bin_map)\n",
    "        greedy_score  = f.calculate_avg_interaction_strength(contact_matrix_zero, top_nodes)\n",
    "        interaction_scores.append(greedy_score)\n",
    "\n",
    "    return interaction_scores\n",
    "\n",
    "\n",
    "# ——— parameters & record collection ———\n",
    "clique_sizes = [5, 10, 20]\n",
    "records = []\n",
    "\n",
    "# 1) load precomputed “strong”/“weak” bg scores from disk\n",
    "# for model_type in [\"strong\", \"weak\"]:\n",
    "#     label = \"Strong\" if model_type == \"strong\" else \"Weak\"\n",
    "#     for size in clique_sizes:\n",
    "#         path = (\n",
    "#             f\"/Users/ethan/Desktop/chromatin-heart-dev/\"\n",
    "#             f\"background_models/greedy_scores_{size}_iterations_1000_{model_type}.txt\"\n",
    "#         )\n",
    "#         with open(path, \"r\") as fh:\n",
    "#             scores = [float(line) for line in fh]\n",
    "#         for s in scores:\n",
    "#             records.append({\n",
    "#                 \"Clique Size\": size,\n",
    "#                 \"Score\": s,\n",
    "#                 \"Model\": label\n",
    "#             })\n",
    "\n",
    "# 2) now call get_top_n_tf_clique for each clique size and append those too\n",
    "for size in clique_sizes:\n",
    "    tf_scores = get_top_n_tf_clique(size)\n",
    "    for s in tf_scores:\n",
    "        records.append({\n",
    "            \"Clique Size\": size,\n",
    "            \"Score\": s,\n",
    "            \"Model\": \"Top TF\"\n",
    "        })\n",
    "\n",
    "# 3) call get_middle_n_tf_clique for each clique size and append those too\n",
    "for size in clique_sizes:\n",
    "    middle_scores = get_middle_n_tf_clique(size)\n",
    "    for s in middle_scores:\n",
    "        records.append({\n",
    "            \"Clique Size\": size,\n",
    "            \"Score\": s,\n",
    "            \"Model\": \"Middle TF\"\n",
    "        })\n",
    "\n",
    "# 4) call bg_greedy_top_bin_tf for each clique size and append those too\n",
    "# for size in clique_sizes:\n",
    "#     bg_scores = bg_greedy_top_bin_tf(contact_matrix_zero, max_bins, size, bin_map)\n",
    "#     for s in bg_scores:\n",
    "#         records.append({\n",
    "#             \"Clique Size\": size,\n",
    "#             \"Score\": s,\n",
    "#             \"Model\": \"Greedy TF Top\"\n",
    "#         })\n",
    "\n",
    "# build the DataFrame\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# ——— plot ———\n",
    "plt.figure(figsize=(14, 5))\n",
    "ax = sns.violinplot(\n",
    "    data=df,\n",
    "    x=\"Clique Size\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Model\",      # will now show Strong, Weak, TF\n",
    "    dodge=True,\n",
    "    width=0.7,\n",
    "    inner=\"quartile\",\n",
    "    cut=0\n",
    ")\n",
    "sns.despine(left=True)\n",
    "ax.set_yscale('symlog', linthresh=0.01)\n",
    "ax.set_title(\"Top vs Middle TF Distributions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import functions as f\n",
    "\n",
    "def get_top_n_tf_clique(clique_size):\n",
    "    # Load the transcription factor binding sites\n",
    "    data_dir = './data/transcription_factors'\n",
    "    tf_files = os.listdir(data_dir)\n",
    "    interaction_scores = []\n",
    "\n",
    "    # Contains top 1 bin for each TF\n",
    "    max_bins = []\n",
    "    bin_set = set()\n",
    "\n",
    "    # Return counts of transcription factor binding sites in each 100kb bin\n",
    "    def process_bed_file(file_path):\n",
    "        bin_counts = {}\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.startswith('#') or not line.strip():\n",
    "                    continue\n",
    "                columns = line.split()\n",
    "                \n",
    "                chrom = columns[0]\n",
    "                start_position = int(columns[1])    \n",
    "                \n",
    "                bin_number = f.find_bin(chrom, start_position, bin_map_loc)\n",
    "                \n",
    "                if bin_number not in bin_counts:\n",
    "                    bin_counts[bin_number] = 0\n",
    "                bin_counts[bin_number] += 1\n",
    "\n",
    "        return bin_counts\n",
    "\n",
    "\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        # open file\n",
    "\n",
    "        try:    \n",
    "            bin_counts = process_bed_file(os.path.join(data_dir, filename))\n",
    "        except:\n",
    "            print(f'Error processing {filename}')\n",
    "            continue\n",
    "\n",
    "        # get top 20 bins\n",
    "        top_bins = sorted(bin_counts, key=bin_counts.get, reverse=True)[:clique_size]\n",
    "\n",
    "        max_bins.append(top_bins[0])\n",
    "        bin_set.update(top_bins)\n",
    "\n",
    "        interaction_strength = f.calculate_avg_interaction_strength(contact_matrix_zero, top_bins)\n",
    "        interaction_scores.append(interaction_strength)\n",
    "\n",
    "    return interaction_scores\n",
    "\n",
    "def get_middle_n_tf_clique(clique_size):\n",
    "    # Load the transcription factor binding sites\n",
    "    data_dir = './data/transcription_factors'\n",
    "    tf_files = os.listdir(data_dir)\n",
    "    interaction_scores = []\n",
    "\n",
    "    # Contains middle bins for each TF\n",
    "    middle_bins = []\n",
    "    bin_set = set()\n",
    "\n",
    "    # Return counts of transcription factor binding sites in each 100kb bin\n",
    "    def process_bed_file(file_path):\n",
    "        bin_counts = {}\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.startswith('#') or not line.strip():\n",
    "                    continue\n",
    "                columns = line.split()\n",
    "                \n",
    "                chrom = columns[0]\n",
    "                start_position = int(columns[1])    \n",
    "                \n",
    "                bin_number = f.find_bin(chrom, start_position, bin_map_loc)\n",
    "                \n",
    "                if bin_number not in bin_counts:\n",
    "                    bin_counts[bin_number] = 0\n",
    "                bin_counts[bin_number] += 1\n",
    "\n",
    "        return bin_counts\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        try:    \n",
    "            bin_counts = process_bed_file(os.path.join(data_dir, filename))\n",
    "        except:\n",
    "            print(f'Error processing {filename}')\n",
    "            continue\n",
    "\n",
    " \n",
    "        sorted_bins = sorted(bin_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        total_bins = len(sorted_bins)\n",
    "        \n",
    "        start_index = (total_bins - clique_size) // 2\n",
    "        if start_index < 0:  \n",
    "            start_index = 0\n",
    "        \n",
    "        middle_n_bins = [bin_num for bin_num, _ in sorted_bins[start_index:start_index + clique_size]]\n",
    "\n",
    "        \n",
    "        if len(middle_n_bins) < clique_size:\n",
    "            middle_n_bins = [bin_num for bin_num, _ in sorted_bins]\n",
    "        \n",
    "        if middle_n_bins: \n",
    "            middle_bins.append(middle_n_bins[0])\n",
    "            bin_set.update(middle_n_bins)\n",
    "\n",
    "            interaction_strength = f.calculate_avg_interaction_strength(contact_matrix_zero, middle_n_bins)\n",
    "            interaction_scores.append(interaction_strength)\n",
    "\n",
    "    return interaction_scores\n",
    "\n",
    "def bg_greedy_top_bin_tf(contact_matrix_zero, max_bins, clique_size, bin_map):\n",
    "    interaction_scores = []\n",
    "    for bin in tqdm(max_bins):\n",
    "        top_nodes = f.find_clique_greedy_fast(contact_matrix_zero, clique_size, bin)\n",
    "        greedy_score  = f.calculate_avg_interaction_strength(contact_matrix_zero, top_nodes)\n",
    "        interaction_scores.append(greedy_score)\n",
    "\n",
    "    return interaction_scores\n",
    "\n",
    "\n",
    "# ——— parameters & record collection ———\n",
    "clique_sizes = [5, 10, 20]\n",
    "records = []\n",
    "\n",
    "# 1) load precomputed “strong”/“weak” bg scores from disk\n",
    "for model_type in [\"strong\", \"weak\"]:\n",
    "    label = \"Strong\" if model_type == \"strong\" else \"Weak\"\n",
    "    for size in clique_sizes:\n",
    "        path = (\n",
    "            f\"/Users/ethan/Desktop/chromatin-heart-dev/\"\n",
    "            f\"background_models/greedy_scores_{size}_iterations_1000_{model_type}.txt\"\n",
    "        )\n",
    "        with open(path, \"r\") as fh:\n",
    "            scores = [float(line) for line in fh]\n",
    "        for s in scores:\n",
    "            records.append({\n",
    "                \"Clique Size\": size,\n",
    "                \"Score\": s,\n",
    "                \"Model\": label\n",
    "            })\n",
    "\n",
    "# 2) now call get_top_n_tf_clique for each clique size and append those too\n",
    "for size in clique_sizes:\n",
    "    tf_scores = get_top_n_tf_clique(size)\n",
    "    for s in tf_scores:\n",
    "        records.append({\n",
    "            \"Clique Size\": size,\n",
    "            \"Score\": s,\n",
    "            \"Model\": \"Top TF\"\n",
    "        })\n",
    "\n",
    "# 3) call get_middle_n_tf_clique for each clique size and append those too\n",
    "for size in clique_sizes:\n",
    "    middle_scores = get_middle_n_tf_clique(size)\n",
    "    for s in middle_scores:\n",
    "        records.append({\n",
    "            \"Clique Size\": size,\n",
    "            \"Score\": s,\n",
    "            \"Model\": \"Middle TF\"\n",
    "        })\n",
    "\n",
    "# 4) call bg_greedy_top_bin_tf for each clique size and append those too\n",
    "for size in clique_sizes:\n",
    "    bg_scores = bg_greedy_top_bin_tf(contact_matrix_zero, max_bins, size, bin_map)\n",
    "    for s in bg_scores:\n",
    "        records.append({\n",
    "            \"Clique Size\": size,\n",
    "            \"Score\": s,\n",
    "            \"Model\": \"Greedy TF Top\"\n",
    "        })\n",
    "\n",
    "# build the DataFrame\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# ——— plot ———\n",
    "plt.figure(figsize=(14, 5))\n",
    "ax = sns.violinplot(\n",
    "    data=df,\n",
    "    x=\"Clique Size\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Model\",      # will now show Strong, Weak, TF\n",
    "    dodge=True,\n",
    "    width=0.7,\n",
    "    inner=\"quartile\",\n",
    "    cut=0\n",
    ")\n",
    "sns.despine(left=True)\n",
    "ax.set_yscale('symlog', linthresh=0.01)\n",
    "ax.set_title(\"Top vs Middle TF Distributions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Violin Plot Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import PolyCollection\n",
    "\n",
    "\n",
    "clique_sizes = [5, 10, 20]\n",
    "models       = [\"strong\", \"weak\"]\n",
    "records      = []\n",
    "greedy_scores = []\n",
    "\n",
    "# 2) Compute the real interaction strength once per size,\n",
    "#    and load each background distribution\n",
    "for size in clique_sizes:\n",
    "    # compute on your real contact matrix\n",
    "    top_nodes = f.find_clique_greedy(contact_matrix_zero, size, TTN_BIN, bin_map)\n",
    "    gs = f.calculate_avg_interaction_strength(contact_matrix_zero, top_nodes)\n",
    "    greedy_scores.append(gs)\n",
    "\n",
    "    # load the null distributions\n",
    "    for m in models:\n",
    "        path = (\n",
    "            f\"/Users/ethan/Desktop/chromatin-heart-dev/\"\n",
    "            f\"background_models/greedy_scores_{size}_iterations_1000_{m}.txt\"\n",
    "        )\n",
    "        with open(path, \"r\") as bg:\n",
    "            bg_scores = [float(line) for line in bg]\n",
    "        for s in bg_scores:\n",
    "            records.append({\n",
    "                \"Clique Size\": size,\n",
    "                \"Score\":       s,\n",
    "                \"Model\":       m.capitalize()\n",
    "            })\n",
    "\n",
    "# 3) Build a DataFrame\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# 4) Draw the side‐by‐side violins\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.violinplot(\n",
    "    data=df,\n",
    "    x=\"Clique Size\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Model\",\n",
    "    dodge=True,\n",
    "    inner=\"quartile\",\n",
    "    cut=0,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# 5) Overlay a horizontal line at each greedy_score\n",
    "bodies   = [c for c in ax.collections if isinstance(c, PolyCollection)]\n",
    "n_models = len(models)  # 2 in this case\n",
    "\n",
    "for j, size in enumerate(clique_sizes):\n",
    "    # the two violins for this size occupy positions j*2 and j*2+1 in `bodies`\n",
    "    group = bodies[j*n_models:(j+1)*n_models]\n",
    "\n",
    "    # collect all x‐coords across both violins\n",
    "    xs = []\n",
    "    for body in group:\n",
    "        verts = body.get_paths()[0].vertices\n",
    "        xs.extend(verts[:,0])\n",
    "    x_min, x_max = min(xs), max(xs)\n",
    "\n",
    "    y = greedy_scores[j]\n",
    "    # draw the line\n",
    "    ax.hlines(y, x_min, x_max, colors=\"red\", linestyles=\"--\")\n",
    "    # annotate the exact value\n",
    "    ax.text(\n",
    "        (x_min + x_max)/2, y,\n",
    "        y,\n",
    "        ha=\"center\", va=\"bottom\",\n",
    "        color=\"red\", fontweight=\"bold\"\n",
    "    )\n",
    "\n",
    "sns.despine(left=True)\n",
    "plt.title(\"Strong vs. Weak Background Distributions\\nwith Real Interaction Strength\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import PolyCollection\n",
    "\n",
    "clique_sizes = [5, 10, 20]\n",
    "models = [\"strong\", \"weak\"]\n",
    "\n",
    "greedy_scores = [\n",
    "    f.calculate_avg_interaction_strength(\n",
    "        contact_matrix_zero,\n",
    "        f.find_clique_greedy(contact_matrix_zero, s, TTN_BIN, bin_map)\n",
    "    )\n",
    "    for s in clique_sizes\n",
    "]\n",
    "\n",
    "records = []\n",
    "for s in clique_sizes:\n",
    "    for m in models:\n",
    "        path = f\"/Users/ethan/Desktop/chromatin-heart-dev/background_models/greedy_scores_{s}_iterations_1000_{m}.txt\"\n",
    "        scores = [float(x) for x in open(path)]\n",
    "        records += [{\"Clique Size\": s, \"Score\": v, \"Model\": m.capitalize()} for v in scores]\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "for i, s in enumerate(clique_sizes):\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    sub = df[df[\"Clique Size\"] == s]\n",
    "    sns.violinplot(data=sub, x=\"Model\", y=\"Score\", inner=\"quartile\", cut=0, ax=ax)\n",
    "    bodies = [c for c in ax.collections if isinstance(c, PolyCollection)]\n",
    "    xs = [v for body in bodies for v in body.get_paths()[0].vertices[:, 0]]\n",
    "    y = greedy_scores[i]\n",
    "    ax.hlines(y, min(xs), max(xs), color=\"red\", linestyle=\"--\")\n",
    "    ax.text((min(xs) + max(xs)) / 2, y, f\"{y:.6f}\", ha=\"center\", va=\"bottom\", color=\"red\", fontweight=\"bold\")\n",
    "    ax.set_title(f\"Clique Size = {s}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clique_sizes = [3, 4, 5, 8, 10, 12, 15, 20]\n",
    "p_values = []\n",
    "\n",
    "for i in clique_sizes:\n",
    "    with open(f'/Users/ethan/Desktop/chromatin-heart-dev/background_models/greedy_scores_{i}_iterations_1000.txt', 'r') as bg:\n",
    "        bg_scores = bg.read().splitlines()\n",
    "        bg_scores = [float(score) for score in bg_scores]\n",
    "\n",
    "        top_nodes = f.find_clique_greedy(contact_matrix_zero, i, TTN_BIN, bin_map)\n",
    "        greedy_score = f.calculate_avg_interaction_strength(contact_matrix_zero, top_nodes)\n",
    "\n",
    "        p_value = t_test(greedy_score, bg_scores)\n",
    "        p_values.append(p_value)\n",
    "\n",
    "\n",
    "plt.plot(clique_sizes, p_values, marker='o')\n",
    "plt.xlabel('Clique Size')\n",
    "plt.ylabel('p-value')\n",
    "plt.yscale('log')  # Useful if values vary widely\n",
    "plt.axhline(0.05, color='r', linestyle='--', label='Significance Threshold (0.05)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sizes_strong = [5, 10, 20, 40]\n",
    "sizes_weak   = [5, 10, 20]\n",
    "\n",
    "# Compute strong p‑values\n",
    "p_strong = []\n",
    "for size in sizes_strong:\n",
    "    with open(\n",
    "        f'/Users/ethan/Desktop/chromatin-heart-dev/'\n",
    "        f'background_models/greedy_scores_{size}_iterations_1000_strong.txt',\n",
    "        'r'\n",
    "    ) as strong_bg_file:\n",
    "        bg_scores_strong = [float(line) for line in strong_bg_file]\n",
    "\n",
    "    nodes = f.find_clique_greedy(contact_matrix_zero, size, TTN_BIN, bin_map)\n",
    "    score = f.calculate_avg_interaction_strength(contact_matrix_zero, nodes)\n",
    "    p_strong.append(empirical_p_value(score, bg_scores_strong))\n",
    "\n",
    "# Compute weak p‑values\n",
    "p_weak = []\n",
    "for size in sizes_weak:\n",
    "    with open(\n",
    "        f'/Users/ethan/Desktop/chromatin-heart-dev/'\n",
    "        f'background_models/greedy_scores_{size}_iterations_1000_weak.txt',\n",
    "        'r'\n",
    "    ) as weak_bg_file:\n",
    "        bg_scores_weak = [float(line) for line in weak_bg_file]\n",
    "\n",
    "    # reuse the same nodes/score or recompute if you prefer:\n",
    "    nodes = f.find_clique_greedy(contact_matrix_zero, size, TTN_BIN, bin_map)\n",
    "    score = f.calculate_avg_interaction_strength(contact_matrix_zero, nodes)\n",
    "    p_weak.append(empirical_p_value(score, bg_scores_weak))\n",
    "\n",
    "# Plot both series\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(sizes_strong, p_strong, marker='o', linestyle='-', label='Strong')\n",
    "plt.plot(sizes_weak,   p_weak,   marker='s', linestyle='--', label='Weak')\n",
    "\n",
    "plt.xlabel('Clique Size')\n",
    "plt.ylabel('p-value')\n",
    "plt.yscale('log')\n",
    "plt.axhline(0.05, color='r', linestyle='--', label='α = 0.05')\n",
    "\n",
    "# Annotate points\n",
    "for x, y in zip(sizes_strong, p_strong):\n",
    "    plt.text(x, y, f\"{y:.1e}\", ha='center', va='bottom', fontsize=8)\n",
    "for x, y in zip(sizes_weak, p_weak):\n",
    "    plt.text(x, y, f\"{y:.1e}\", ha='center', va='top',    fontsize=8)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_from_bg_model(type, clique_size, iterations):\n",
    "    file_suffix = 'iterations' if type == 'greedy' else 'molecules'\n",
    "    file_path = f'/Users/ethan/Desktop/chromatin-heart-dev/background_models/{type}_scores_{clique_size}_{file_suffix}_{iterations}.txt'\n",
    "    \n",
    "    with open(file_path, 'r') as bg:\n",
    "        bg_scores = [float(score) for score in bg.read().splitlines()]\n",
    "    \n",
    "    top_nodes = f.find_clique_greedy(contact_matrix_zero, clique_size, TTN_BIN, bin_map)\n",
    "    greedy_score = f.calculate_avg_interaction_strength(contact_matrix_zero, top_nodes)\n",
    "\n",
    "    mean_bg = np.mean(bg_scores)\n",
    "    var_bg = np.var(bg_scores)\n",
    "    std_bg = np.std(bg_scores)\n",
    "\n",
    "    p_value = mann_whitney_u_test(greedy_score, bg_scores)\n",
    "    percent_higher = np.mean(np.array(bg_scores) > greedy_score) * 100\n",
    "\n",
    "    print(f'TTN Clique Score: {greedy_score:.4f}')\n",
    "    print(f'Background Scores Mean: {mean_bg:.4f}')\n",
    "    print(f'Background Scores Variance: {var_bg:.4f}')\n",
    "    print(f'Background Scores Std Dev: {std_bg:.4f}')\n",
    "    print(f'Percentage of Background Scores Higher Than TTN: {percent_higher:.2f}%')\n",
    "    print(f'p-value: {p_value:.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_from_bg_model('greedy', 5, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_from_bg_model('greedy', 8, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Given values\n",
    "ttn_score = 0.0051\n",
    "mean_bg = 0.0030\n",
    "std_bg = 0.0019\n",
    "\n",
    "# Z-score calculation\n",
    "z_score = (ttn_score - mean_bg) / std_bg\n",
    "\n",
    "# P-value calculation from Z-score (one-tailed test)\n",
    "p_value_from_z = 1 - norm.cdf(z_score)\n",
    "\n",
    "print(f'Z-score: {z_score:.2f}')\n",
    "print(f'P-value from Z-score: {p_value_from_z:.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha selection\n",
    "\n",
    "To investigate the most optimal alpha for building a clique around TTN, a comparison between strong/weak distributions will be made, looking at how the fold change between the median and mann whitney p value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk_fast(contact_matrix, start_node, n,\n",
    "                     neighbors, cdfs,\n",
    "                     num_molecules=100, alpha=0.1):\n",
    "    \"\"\"\n",
    "    A much faster random‐walk using prebuilt neighbor/CDF lists.\n",
    "    \"\"\"\n",
    "    N = contact_matrix.shape[0]\n",
    "    visit_count = np.zeros(N, dtype=int)\n",
    "\n",
    "    for _ in tqdm(range(num_molecules)):\n",
    "        cur = start_node\n",
    "        while True:\n",
    "            visit_count[cur] += 1\n",
    "            if np.random.rand() < alpha or neighbors[cur].size == 0:\n",
    "                break\n",
    "            r = np.random.rand()\n",
    "            # find next index in CDF\n",
    "            j = np.searchsorted(cdfs[cur], r, side='right')\n",
    "            cur = neighbors[cur][j]\n",
    "\n",
    "    # top-n visited\n",
    "    return np.argsort(visit_count)[-n:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clique = random_walk_fast(\n",
    "    contact_matrix_zero, start_node=4275, n=5,\n",
    "    neighbors=neighbors, cdfs=cdfs,\n",
    "    num_molecules=5000000, alpha=0.1\n",
    ")\n",
    "print(clique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as f\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "clique_size     = 10\n",
    "num_iterations  = 1000\n",
    "alphas          = [0.01, 0.03, 0.05, 0.1, 0.25, 0.5, 0.75, 0.99]\n",
    "\n",
    "fold_changes = []\n",
    "p_values     = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    bg_strong = f.create_background_model_rw(\n",
    "        contact_matrix_zero,\n",
    "        clique_size,\n",
    "        bins=gene_bins,\n",
    "        label='strong',\n",
    "        num_molecules=1000,\n",
    "        neighbors=neighbors,\n",
    "        cdfs=cdfs,\n",
    "        num_iterations=num_iterations,\n",
    "        alpha=alpha,\n",
    "        plot=False\n",
    "    )\n",
    "    bg_weak = f.create_background_model_rw(\n",
    "        contact_matrix_zero,\n",
    "        clique_size,\n",
    "        bins=non_gene_bins,\n",
    "        label='weak',\n",
    "        num_molecules=1000,\n",
    "        neighbors=neighbors,\n",
    "        cdfs=cdfs,\n",
    "        num_iterations=num_iterations,\n",
    "        alpha=alpha,\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    fold_change = np.median(bg_strong) / np.median(bg_weak)\n",
    "    _, p_value  = mannwhitneyu(bg_strong, bg_weak, alternative='greater')\n",
    "\n",
    "    fold_changes.append(fold_change)\n",
    "    p_values.append(p_value)\n",
    "\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), sharex=True)\n",
    "\n",
    "# fold-Change vs alpha\n",
    "ax1.plot(alphas, fold_changes, marker='o')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xticks(alphas)\n",
    "ax1.get_xaxis().set_major_formatter(ScalarFormatter())\n",
    "ax1.set_xlabel('Alpha Value')\n",
    "ax1.set_ylabel('Fold Change')\n",
    "ax1.set_title('Fold Change vs Alpha')\n",
    "ax1.grid(True)\n",
    "\n",
    "# p-value vs alpha with p=0.05 line\n",
    "ax2.plot(alphas, p_values, marker='s')\n",
    "ax2.axhline(0.05, linestyle='--', color='red', label='p = 0.05')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xticks(alphas)\n",
    "ax2.get_xaxis().set_major_formatter(ScalarFormatter())\n",
    "ax2.set_xlabel('Alpha Value')\n",
    "ax2.set_ylabel('Mann–Whitney p-Value')\n",
    "ax2.set_title('p-Value vs Alpha')\n",
    "\n",
    "\n",
    "max_p = max(p_values)\n",
    "top = max(max_p * 1.1, 0.05)\n",
    "ax2.set_ylim(0, top)\n",
    "\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Molecules vs Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# plot molecules to time\n",
    "molecules = [100,1000,10000,25000,50000, 100000, 250000]\n",
    "time_values = []\n",
    "\n",
    "for m in molecules:\n",
    "    start = time.time()\n",
    "    top_nodes = f.random_walk(contact_matrix_zero, TTN_BIN, 5, num_molecules=m, alpha=0.05, verbose=True)\n",
    "    end = time.time()\n",
    "    time_values.append(end-start)\n",
    "\n",
    "plt.plot(molecules, time_values, marker='o')\n",
    "plt.xlabel('Number of Molecules')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha vs Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot alpha values to time using clique size 5\n",
    "alphas = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "time_values = []\n",
    "\n",
    "for a in alphas:\n",
    "    start = time.time()\n",
    "    top_nodes = f.random_walk(contact_matrix_zero, TTN_BIN, 5, num_molecules=1000, alpha=a, verbose=True)\n",
    "    end = time.time()\n",
    "    time_values.append(end-start)\n",
    "\n",
    "plt.plot(alphas, time_values, marker='o')\n",
    "plt.xlabel('Alpha Value')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha vs p value, (for now can use clique size 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alphas = [0.1, 0.2, 0.3, 0.5, 0.99]\n",
    "p_values = []\n",
    "\n",
    "for a in alphas:\n",
    "    top_nodes = f.random_walk(contact_matrix_zero, TTN_BIN, 5, num_molecules=1000, alpha=a, verbose=True)\n",
    "    rw_score = f.calculate_avg_interaction_strength(contact_matrix_zero, top_nodes)\n",
    "    bg_model_rw = f.create_background_model_rw_strong(contact_matrix_zero, 5, gene_bins=gene_bins, num_molecules=1000, num_iterations=1000, alpha=a)\n",
    "\n",
    "    p_value = mann_whitney_u_test(rw_score, bg_model_rw)\n",
    "    p_values.append(p_value)\n",
    "\n",
    "plt.plot(alphas, p_values, marker='o')\n",
    "plt.xlabel('Reset Probability (alpha)')\n",
    "plt.ylabel('p-value')\n",
    "plt.yscale('log')  # Log scale if values vary widely\n",
    "plt.axhline(0.05, color='r', linestyle='--', label='Significance Threshold (0.05)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bed Narrow Peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as f\n",
    "# LOAD BIN MAP\n",
    "bin_map_loc = f.load_bin_map_loc('/Users/ethan/Desktop/chromatin-heart-dev/data/bin_map_human_100000.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.find_bin('chr2', 61443502, bin_map_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data/transcription_factors, contains a directory of .bed files containing transcription factor binding sites\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the transcription factor binding sites\n",
    "data_dir = './data/transcription_factors'\n",
    "tf_files = os.listdir(data_dir)\n",
    "\n",
    "# Return counts of transcription factor binding sites in each 100kb bin\n",
    "def process_bed_file(file_path):\n",
    "    bin_counts = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('#') or not line.strip():\n",
    "                continue\n",
    "            columns = line.split()\n",
    "            \n",
    "            chrom = columns[0]\n",
    "            start_position = int(columns[1])    \n",
    "            \n",
    "            bin_number = f.find_bin(chrom, start_position, bin_map_loc)\n",
    "            \n",
    "            if bin_number not in bin_counts:\n",
    "                bin_counts[bin_number] = 0\n",
    "            bin_counts[bin_number] += 1\n",
    "\n",
    "    return bin_counts\n",
    "\n",
    "interaction_scores = []\n",
    "\n",
    "# Contains top 1 bin for each TF\n",
    "max_bins = []\n",
    "bin_set = set()\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    # open file\n",
    "\n",
    "    try:    \n",
    "        bin_counts = process_bed_file(os.path.join(data_dir, filename))\n",
    "    except:\n",
    "        print(f'Error processing {filename}')\n",
    "        continue\n",
    "\n",
    "    # get top 20 bins\n",
    "    top_bins = sorted(bin_counts, key=bin_counts.get, reverse=True)[:20]\n",
    "\n",
    "    max_bins.append(top_bins[0])\n",
    "    bin_set.update(top_bins)\n",
    "\n",
    "    interaction_strength = f.calculate_avg_interaction_strength(contact_matrix_zero, top_bins)\n",
    "    interaction_scores.append(interaction_strength)\n",
    "\n",
    "    # get span of chromosmes of the top bins\n",
    "    chrom_span = f.get_chromosome_span(top_bins, bin_map)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the distribution as a violin plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(y=interaction_scores, inner='box', color='skyblue')\n",
    "plt.ylabel('Average Interaction Score')\n",
    "plt.title('Distribution of Interaction Strength for TF-Clique')\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(interaction_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_scores = []\n",
    "\n",
    "for i in tqdm(range(len(max_bins))):\n",
    "    clique = f.find_clique_greedy(contact_matrix_zero, 5, max_bins[i], bin_map)\n",
    "    score = f.calculate_avg_interaction_strength(contact_matrix_zero, clique)\n",
    "    interaction_scores.append(score)\n",
    "\n",
    "# Plot the distribution of interaction scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(interaction_scores, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Average Interaction Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Distribution of Interaction Strength for TF-Clique')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_scores = []\n",
    "\n",
    "for i in tqdm(range(len(max_bins))):\n",
    "    clique = f.random_walk(contact_matrix_zero, max_bins[i], 5, num_molecules=1000, alpha=0.05, verbose=False)\n",
    "    score = f.calculate_avg_interaction_strength(contact_matrix_zero, clique)\n",
    "    interaction_scores.append(score)\n",
    "\n",
    "# Plot the distribution of interaction scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(interaction_scores, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Average Interaction Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Distribution of Interaction Strength for TF-Clique')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttn_clique = f.random_walk(contact_matrix_zero, TTN_BIN, 5, num_molecules=1000, alpha=0.05, verbose=False)\n",
    "score_rw = f.calculate_avg_interaction_strength(contact_matrix_zero, ttn_clique)\n",
    "\n",
    "t_test(score_rw, interaction_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genes from TTN Clique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4275, 4276, 4277, 4278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_clique = f.find_clique_greedy(contact_matrix_zero, 40, TTN_BIN, bin_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdasdsas = f.find_clique_greedy(contact_matrix_zero, 40, TTN_BIN, bin_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_clique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "TTN_GENES = set()\n",
    "for bin in tqdm(top_40_clique):\n",
    "    genes = f.find_gene_from_bin(bin, '/Users/ethan/Desktop/chromatin-heart-dev/data/bin_map_human_100000.bed', '/Users/ethan/Desktop/chromatin-heart-dev/data/gencode.v38.annotation.gtf')\n",
    "    TTN_GENES.update(genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_clique_rw = f.random_walk(contact_matrix_zero, TTN_BIN, 40, num_molecules=1000, alpha=0.05, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_clique_rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTN_GENES_RW = set()    \n",
    "\n",
    "for bin in tqdm(top_40_clique_rw):\n",
    "    genes = f.find_gene_from_bin(bin, '/Users/ethan/Desktop/chromatin-heart-dev/data/bin_map_human_100000.bed', '/Users/ethan/Desktop/chromatin-heart-dev/data/gencode.v38.annotation.gtf')\n",
    "    TTN_GENES_RW.update(genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TTN_GENES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write list to file\n",
    "\n",
    "with open('/Users/ethan/Desktop/chromatin-heart-dev/TTN_genes.txt', 'w') as tg:\n",
    "    for item in TTN_GENES:\n",
    "        tg.write(\"%s\\n\" % item)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as f\n",
    "\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "\n",
    "\n",
    "f.random_walk_fast(\n",
    "    contact_matrix_zero, TTN_BIN, 5,\n",
    "    neighbors=neighbors, cdfs=cdfs,\n",
    "    num_molecules=10000, alpha=0.05\n",
    ")\n",
    "end = time()\n",
    "\n",
    "print(f\"Time taken: {end - start} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
